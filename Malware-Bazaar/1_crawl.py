import requests
from bs4 import BeautifulSoup
import os
import time
import random

# 기본 URL 설정
BASE_URL = "https://bazaar.abuse.ch/browse/"
DOWNLOAD_URL = "https://bazaar.abuse.ch"

# 요청 헤더 설정
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.6613.120 Safari/537.36',
    'Accept-Language': 'ko-KR,ko;q=0.9',
    'Referer': 'https://bazaar.abuse.ch/verify-ua/',
    'Cache-Control': 'max-age=0',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Ch-Ua': '"Not;A=Brand";v="24", "Chromium";v="128"',
    'Sec-Ch-Ua-Mobile': '?0',
    'Sec-Ch-Ua-Platform': '"Windows"',
    'Sec-Fetch-Site': 'same-origin',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-User': '?1',
    'Sec-Fetch-Dest': 'document',
    'Accept-Encoding': 'gzip, deflate, br'
}

def random_seed():
    now = time.localtime()
    tmp1 = float(time.strftime('%S', now))
    tmp2 = float(time.strftime('%M', now))
    sec = tmp1 * tmp2
    sec -= sec
    sec = sec * 100 // 10
    random.seed(sec)

# PE 파일 식별 함수
def is_pe_file(tag):
    # PE 파일을 나타내는 태그 식별
    return tag.find('img', {'src': '/images/icons/application.png'}) and tag.get_text(strip=True) == 'exe'

# 크롤링 함수 정의
def download_pe_files():
    for page_idx in range(1, 5):  # 1페이지부터 4페이지까지 반복
        print(f"{page_idx}페이지 요청 중...")
        # 페이지 요청
        page_url = f"{BASE_URL}?dt_idx={page_idx}"
        response = requests.get(page_url, headers=headers)
        response.raise_for_status()
        
        # BeautifulSoup로 HTML 파싱
        print("HTML 파싱 중...")
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # PE 파일 링크 수집
        print("PE 파일 링크 수집 중...")
        for tag in soup.find_all('td'):
            random_seed()
            if is_pe_file(tag):
                download_link = tag.find_next('a', class_='badge badge-secondary')
                
                if download_link:
                    # 해시 값 추출
                    file_hash = download_link['href'].split('/')[-2]
                    file_page_url = f"{DOWNLOAD_URL}/download/{file_hash}/"
                    
                    print(f"파일 해시: {file_hash}, 파일 페이지 URL: {file_page_url}")
                    
                    # 각 파일의 다운로드 페이지로 이동
                    print("파일 다운로드 페이지 요청 중...")
                    file_page_response = requests.get(file_page_url, headers=headers)
                    file_page_response.raise_for_status()
                    file_page_soup = BeautifulSoup(file_page_response.text, 'html.parser')
                    
                    # 다운로드 버튼의 ID와 value 속성에서 키 추출
                    download_button = file_page_soup.find('button', {'class': 'btn btn-primary'})
                    if download_button:
                        file_key = download_button['value']  # 파일 키
                        
                        # 최종 다운로드 URL 생성
                        final_download_url = f"{DOWNLOAD_URL}/download/{file_key}/"
                        print(f"최종 다운로드 URL: {final_download_url}")
                        
                        # 파일 다운로드
                        print("파일 다운로드 중...")
                        file_content = requests.get(final_download_url, headers=headers).content
                        
                        # 파일 저장
                        file_name = f"sample/{file_hash}.zip"
                        with open(file_name, 'wb') as file:
                            file.write(file_content)
                        
                        print(f"{file_name} 다운로드 완료.")
                        
                        # 서버에 부담을 주지 않도록 약간의 지연 추가
                        time.sleep(random.uniform(0.8,2.0))

# 크롤링 함수 실행
download_pe_files()

